@article{searle2021estimating,
  author    = {Searle, T. and Ibrahim, Z. and Teo, J. and Dobson, R.},
  title     = {Estimating redundancy in clinical text},
  journal   = {Journal of Biomedical Informatics},
  volume    = {124},
  pages     = {103938},
  year      = {2021},
  doi       = {10.1016/j.jbi.2021.103938}
}


@inproceedings{grail2021globalizing,
  author    = {Grail, Q. and Perez, J. and Gaussier, E.},
  title     = {Globalizing BERT-based transformer architectures for long document summarization},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  editor    = {Merlo, P. and Tiedemann, J. and Tsarfaty, R.},
  pages     = {1792--1810},
  year      = {2021},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2021.eacl-main.154}
}

@article{frey2014ehr,
  title={EHR big data deep phenotyping},
  author={Frey, LJ and Lenert, L and Lopez-Campos, G},
  journal={Yearbook of medical informatics},
  volume={23},
  number={01},
  pages={206--211},
  year={2014},
  publisher={Georg Thieme Verlag KG}
}
@article{cyganek2016survey,
  title={A survey of big data issues in electronic health record analysis},
  author={Cyganek, Bogus{\l}aw and Gra{\~n}a, Manuel and Krawczyk, Bartosz and Kasprzak, Andrzej and Porwik, Piotr and Walkowiak, Krzysztof and Wo{\'z}niak, Micha{\l}},
  journal={Applied Artificial Intelligence},
  volume={30},
  number={6},
  pages={497--520},
  year={2016},
  publisher={Taylor \& Francis}
}
@article{chopra2013natural,
  title={Natural language processing},
  author={Chopra, Abhimanyu and Prashar, Abhinav and Sain, Chandresh},
  journal={International journal of technology enhancements and emerging engineering research},
  volume={1},
  number={4},
  pages={131--134},
  year={2013},
  publisher={Citeseer}
}
@article{gillum2013papyrus,
  title={From papyrus to the electronic tablet: a brief history of the clinical medical record with lessons for the digital age},
  author={Gillum, Richard F},
  journal={The American journal of medicine},
  volume={126},
  number={10},
  pages={853--857},
  year={2013},
  publisher={Elsevier}
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  volume={1},
  number={2},
  year={2023}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{cordonnier2020multi,
  title={Multi-head attention: Collaborate instead of concatenate},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal={arXiv preprint arXiv:2006.16362},
  year={2020}
}
@article{ren2024learning,
  title={Learning dynamics of llm finetuning},
  author={Ren, Yi and Sutherland, Danica J},
  journal={arXiv preprint arXiv:2407.10490},
  year={2024}
}
@article{sreenivasllm,
  title={LLM Pruning and Distillation in Practice},
  author={Sreenivas, Sharath Turuvekere and Muralidharan, Saurav and Joshi, Raviraj Bhuminand and Chochowski, Marcin and Patwary, Mostofa and Molchanov, Pavlo and Shoeybi, Mohammad and Kautz, Jan and Mahabaleshwarkar, Ameya Sunil and Shen, Gerald and others},
  journal={ICLR},
  year={2025}
}
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@misc{ryu2024keyelementinformedsllmtuningdocument,
      title={Key-Element-Informed sLLM Tuning for Document Summarization}, 
      author={Sangwon Ryu and Heejin Do and Yunsu Kim and Gary Geunbae Lee and Jungseul Ok},
      year={2024},
      eprint={2406.04625},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.04625}, 
}
@InProceedings{10.1007/978-981-99-9864-7_17,
author="Zhao, Bingfei
and Zan, Hongying
and Niu, Chengzhi
and Chang, Hongyang
and Zhang, Kunli",
editor="Xu, Hua
and Chen, Qingcai
and Lin, Hongfei
and Wu, Fei
and Liu, Lei
and Tang, Buzhou
and Hao, Tianyong
and Huang, Zhengxing",
title="Automatic Generation of Discharge Summary of EMRs Based on Multi-granularity Information Fusion",
booktitle="Health Information Processing",
year="2024",
publisher="Springer Nature Singapore",
address="Singapore",
pages="254--269",
abstract="Discharge summaries are a significant component of electronic medical records, playing a crucial role in follow-up treatment and scientific research. However, there are few researches on automated discharge summary generation based on deep learning, and there is also a lack of available datasets. To address this, in this paper, we construct a small-scale dataset containing various types of entity information for the task of automated discharge summary generation from electronic medical records. In order to make full use of the rich entity information implied in medical records, we design a generation model based on a T5 architecture that encodes various types of entity information and incorporates the information contained in the entities into the encoder using multi-granularity fusion methods. Meanwhile, we use pointer-generator networks to enhance the model's generalization capability. The experimental results show that the proposed dataset is challenging, and compared to the baseline models, the proposed model achieves significant improvements on the evaluation metrics. Additionally, ablation studies further demonstrate that incorporating entity information and pointer-generator networks positively contributes to the summarization quality of the model.",
isbn="978-981-99-9864-7"
}

@misc{zhang2024optimizingautomaticsummarizationlong,
      title={Optimizing Automatic Summarization of Long Clinical Records Using Dynamic Context Extension:Testing and Evaluation of the NBCE Method}, 
      author={Guoqing Zhang and Keita Fukuyama and Kazumasa Kishimoto and Tomohiro Kuroda},
      year={2024},
      eprint={2411.08586},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.08586}, 
}
@misc{zhang2025comprehensivesurveyprocessorientedautomatic,
      title={A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods}, 
      author={Yang Zhang and Hanlei Jin and Dan Meng and Jun Wang and Jinghua Tan},
      year={2025},
      eprint={2403.02901},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.02901}, 
}

@article{MORENOBAREA2025109576,
title = {Named entity recognition for de-identifying Spanish electronic health records},
journal = {Computers in Biology and Medicine},
volume = {185},
pages = {109576},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109576},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524016615},
author = {Francisco J. Moreno-Barea and Guillermo López-García and Héctor Mesa and Nuria Ribelles and Emilio Alba and José M. Jerez and Francisco J. Veredas},
keywords = {Named entity recognition, Natural language processing, De-identification, Electronic health records, Spanish},
abstract = {Background and objectives:
There is an increasing and renewed interest in Electronic Health Records (EHRs) as a substantial information source for clinical decision making. Consequently, automatic de-identification of EHRs is an indispensable task, since their dissociation from personal data is a necessary prerequisite for their dissemination. Nevertheless, the bulk of prior research in this domain has been conducted using English EHRs, given the limited availability of annotated corpora in other languages, including Spanish.
Methods:
In this study, the automatic de-identification of medical documents in Spanish was explored. A private corpus comprising 599 genuine clinical cases was annotated with eight different categories of protected health information. The prediction problem was approached as a named entity recognition task and two deep learning-based methodologies were developed. The first strategy was based on recurrent neural networks (RNN) and the second, an end-to-end approach, was based on Transformers. In addition, we have implemented a procedure to expand the amount of texts employed for model training.
Results:
Our findings demonstrate that Transformers surpass RNNs in the de-identification of clinical data in Spanish. Particularly noteworthy is the excellent performance of the XLM-RoBERTa large Transformer, achieving a rigorous strict-match micro-average of 0.946 for precision, 0.954 for recall, and an F1 score of 0.95 when applied to the amplified version of the corpus. Furthermore, a web-based application has been created to assist specialized clinicians in de-identifying EHRs through the aid of the implemented models.
Conclusion:
The study’s conclusions showcase the practical applicability of the state-of-the-art Transformers models for precise de-identification of clinical notes in real-world medical settings in Spanish, with the potential to improve performance if continual pre-training strategies are implemented.}
}
@misc{davis2025medslicefinetunedlargelanguage,
      title={MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning}, 
      author={Joshua Davis and Thomas Sounack and Kate Sciacca and Jessie M Brain and Brigitte N Durieux and Nicole D Agaronnik and Charlotta Lindvall},
      year={2025},
      eprint={2501.14105},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.14105}, 
}
@misc{guluzade2025elmtexfinetuninglargelanguage,
      title={ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports}, 
      author={Aynur Guluzade and Naguib Heiba and Zeyd Boukhers and Florim Hamiti and Jahid Hasan Polash and Yehya Mohamad and Carlos A Velasco},
      year={2025},
      eprint={2502.05638},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.05638}, 
}
@article{TINN2023100729,
title = {Fine-tuning large neural language models for biomedical natural language processing},
journal = {Patterns},
volume = {4},
number = {4},
pages = {100729},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100729},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000697},
author = {Robert Tinn and Hao Cheng and Yu Gu and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
keywords = {natural language processing, L01.224.050.375.580, biomedical language and understanding benchmark, BLURB},
abstract = {Summary
Large neural language models have transformed modern natural language processing (NLP) applications. However, fine-tuning such models for specific tasks remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP. We conduct a systematic study on fine-tuning stability in biomedical NLP. We show that fine-tuning performance may be sensitive to pretraining settings and conduct an exploration of techniques for addressing fine-tuning instability. We show that these techniques can substantially improve fine-tuning performance for low-resource biomedical NLP applications. Specifically, freezing lower layers is helpful for standard BERT-BASE models, while layerwise decay is more effective for BERT-LARGE and ELECTRA models. For low-resource text similarity tasks, such as BIOSSES, reinitializing the top layers is the optimal strategy. Overall, domain-specific vocabulary and pretraining facilitate robust models for fine-tuning. Based on these findings, we establish a new state of the art on a wide range of biomedical NLP applications.}
}
@misc{saba2024questionansweringbasedsummarizationelectronic,
      title={Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation}, 
      author={Walid Saba and Suzanne Wendelken and James. Shanahan},
      year={2024},
      eprint={2401.01469},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01469}, 
}
@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}
@misc{openaiBatch,
  author       = {OpenAI},
  title        = {Batch Processing - OpenAI API Documentation},
  year         = {2024},
  url          = {https://platform.openai.com/docs/guides/batch},
  note         = {Accedido el 17 de abril de 2025}
}
@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}
@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}
@misc{press2023measuringnarrowingcompositionalitygap,
      title={Measuring and Narrowing the Compositionality Gap in Language Models}, 
      author={Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and Mike Lewis},
      year={2023},
      eprint={2210.03350},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03350}, 
}
@misc{parnami2022learningexamplessummaryapproaches,
      title={Learning from Few Examples: A Summary of Approaches to Few-Shot Learning}, 
      author={Archit Parnami and Minwoo Lee},
      year={2022},
      eprint={2203.04291},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.04291}, 
}
@misc{sun2024promptchainingstepwiseprompt,
      title={Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization}, 
      author={Shichao Sun and Ruifeng Yuan and Ziqiang Cao and Wenjie Li and Pengfei Liu},
      year={2024},
      eprint={2406.00507},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.00507}, 
}
@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040/",
    doi = "10.3115/1073083.1073135",
    pages = "311--318"
}
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}
@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}
@InProceedings{pmlr-v174-pal22a,
	title = 	 {MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
	author =       {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
	booktitle = 	 {Proceedings of the Conference on Health, Inference, and Learning},
	pages = 	 {248--260},
	year = 	 {2022},
	editor = 	 {Flores, Gerardo and Chen, George H and Pollard, Tom and Ho, Joyce C and Naumann, Tristan},
	volume = 	 {174},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {07--08 Apr},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v174/pal22a/pal22a.pdf},
	url = 	 {https://proceedings.mlr.press/v174/pal22a.html},
	abstract = 	 {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects & topics. A detailed explanation of the solution, along with the above information, is provided in this study.}
}

@article{ALKHALAF2024104662,
	title = {Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records},
	journal = {Journal of Biomedical Informatics},
	volume = {156},
	pages = {104662},
	year = {2024},
	issn = {1532-0464},
	doi = {https://doi.org/10.1016/j.jbi.2024.104662},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046424000807},
	author = {Mohammad Alkhalaf and Ping Yu and Mengyang Yin and Chao Deng},
	keywords = {Generative AI, Nursing notes, LLAMA, Malnutrition, Summarization, RAG},
	abstract = {Background
	Malnutrition is a prevalent issue in aged care facilities (RACFs), leading to adverse health outcomes. The ability to efficiently extract key clinical information from a large volume of data in electronic health records (EHR) can improve understanding about the extent of the problem and developing effective interventions. This research aimed to test the efficacy of zero-shot prompt engineering applied to generative artificial intelligence (AI) models on their own and in combination with retrieval augmented generation (RAG), for the automating tasks of summarizing both structured and unstructured data in EHR and extracting important malnutrition information.
	Methodology}
}
@misc{huggingface_4bit,
	title = {Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA},
	author = {Hugging Face},
	year = {2023},
	url = {https://huggingface.co/blog/4bit-transformers-bitsandbytes}
}

@article{hu2021lora,
	title = {LoRA: Low-Rank Adaptation of Large Language Models},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	journal = {arXiv preprint arXiv:2106.09685},
	year = {2021},
	url = {https://arxiv.org/abs/2106.09685}
}

@misc{unsloth_docs,
	title = {Fine-tuning Guide - Unsloth Documentation},
	author = {Unsloth Team},
	year = {2025},
	url = {https://docs.unsloth.ai/get-started/fine-tuning-guide}
}

@misc{huggingface_templates,
	title = {Templates - Hugging Face Transformers},
	author = {Hugging Face},
	year = {2025},
	url = {https://huggingface.co/docs/transformers/main/en/chat_templating}
}
@misc{ruslanmv2024aimedchatbot,
	author       = {Ruslan Magana Vsevolodovna},
	title        = {AI Medical Chatbot},
	year         = {2024},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/ruslanmv/ai-medical-chatbot}},
	note         = {Repositorio de código abierto para un chatbot médico basado en inteligencia artificial}
}
@misc{microsoft_dragon_copilot,
	title = {Conozcan Microsoft Dragon Copilot: su nuevo asistente de IA para el flujo de trabajo clínico},
	howpublished = {\url{https://news.microsoft.com/source/latam/noticias-de-microsoft/conozcan-microsoft-dragon-copilot-su-nuevo-asistente-de-ia-para-el-flujo-de-trabajo-clinico/}},
	year = {2025},
	note = {Accedido el 11 de mayo de 2025}
}
@misc{ji2024llama3deepseek,
	title={Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks}, 
	author={Yuting Guo and Abeed Sarker},
	year={2025},
	eprint={2503.15169},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2503.15169}, 
}
@misc{zhang2024mistral,
title={Enhancing Clinical Efficiency through LLM: Discharge Note Generation for Cardiac Patients}, 
author={HyoJe Jung and Yunha Kim and Heejung Choi and Hyeram Seo and Minkyoung Kim and JiYe Han and Gaeun Kee and Seohyun Park and Soyoung Ko and Byeolhee Kim and Suyeon Kim and Tae Joon Jun and Young-Hak Kim},
year={2024},
eprint={2404.05144},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/2404.05144}, 
}
@ARTICLE{transformers_clinical_coding,
	author={López-García, Guillermo and Jerez, José M. and Ribelles, Nuria and Alba, Emilio and Veredas, Francisco J.},
	journal={IEEE Access}, 
	title={Transformers for Clinical Coding in Spanish}, 
	year={2021},
	volume={9},
	number={},
	pages={72387-72397},
	keywords={Encoding;Task analysis;Neoplasms;Adaptation models;Medical services;Hospitals;Oncology;Clinical coding;deep learning;natural language processing;text classification;transformers},
	doi={10.1109/ACCESS.2021.3080085}}

@inproceedings{clinical_text_classification_cancer,
	author = {Moreno-Barea, Francisco J. and Mesa, H\'{e}ctor and Ribelles, Nuria and Alba, Emilio and Jerez, Jos\'{e} M.},
	title = {Clinical Text Classification in Cancer Real-World Data in Spanish},
	year = {2023},
	isbn = {978-3-031-34952-2},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	url = {https://doi.org/10.1007/978-3-031-34953-9_38},
	doi = {10.1007/978-3-031-34953-9_38},
	abstract = {Healthcare systems currently store a large amount of clinical data, mostly unstructured textual information, such as electronic health records (EHRs). Manually extracting valuable information from these documents is costly for healthcare professionals. For example, when a patient first arrives at an oncology clinical analysis unit, clinical staff must extract information about the type of neoplasm in order to assign the appropriate clinical specialist. Automating this task is equivalent to text classification in natural language processing (NLP). In this study, we have attempted to extract the neoplasm type by processing Spanish clinical documents. A private corpus of 23, 704 real clinical cases has been processed to extract the three most common types of neoplasms in the Spanish territory: breast, lung and colorectal neoplasms. We have developed methodologies based on state-of-the-art text classification task, strategies based on machine learning and bag-of-words, based on embedding models in a supervised task, and based on bidirectional recurrent neural networks with convolutional layers (C-BiRNN). The results obtained show that the application of NLP methods is extremely helpful in performing the task of neoplasm type extraction. In particular, the 2-BiGRU model with convolutional layer and pre-trained fastText embedding obtained the best performance, with a macro-average, more representative than the micro-average due to the unbalanced data, of 0.981 for precision, 0.984 for recall and 0.982 for F1-score.},
	booktitle = {Bioinformatics and Biomedical Engineering: 10th International Work-Conference, IWBBIO 2023, Meloneras, Gran Canaria, Spain, July 12–14, 2023, Proceedings, Part I},
	pages = {482–496},
	numpages = {15},
	keywords = {Text Classification, Natural Language Processing, Electronic Health Records, Neoplasm cancer, Spanish},
	location = {Meloneras, Gran Canaria, Spain}
}

@article{ner_deidentification_ehr,
	author = {Moreno-Barea, Francisco J. and L\'{o}pez-Garc\'{\i}a, Guillermo and Mesa, H\'{e}ctor and Ribelles, Nuria and Alba, Emilio and Jerez, Jos\'{e} M. and Veredas, Francisco J.},
	title = {Named entity recognition for de-identifying Spanish electronic health records},
	year = {2025},
	issue_date = {Feb 2025},
	publisher = {Pergamon Press, Inc.},
	address = {USA},
	volume = {185},
	number = {C},
	issn = {0010-4825},
	url = {https://doi.org/10.1016/j.compbiomed.2024.109576},
	doi = {10.1016/j.compbiomed.2024.109576},
	journal = {Comput. Biol. Med.},
	month = feb,
	numpages = {19},
	keywords = {Named entity recognition, Natural language processing, De-identification, Electronic health records, Spanish}
}
@online{awsRAG,
	author       = {{Amazon Web Services}},
	title        = {What is Retrieval-Augmented Generation (RAG)?},
	year         = {2023},
	url          = {https://aws.amazon.com/what-is/retrieval-augmented-generation/},
	note         = {Accedido el 27 de mayo de 2025}
}
@online{ollama2023,
	author       = {Ollama},
	title        = {Ollama - Get up and running with large language models locally},
	year         = {2023},
	url          = {https://ollama.com},
	note         = {Accedido el 27 de mayo de 2025}
}
@misc{loper2002nltknaturallanguagetoolkit,
	title={NLTK: The Natural Language Toolkit}, 
	author={Edward Loper and Steven Bird},
	year={2002},
	eprint={cs/0205028},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/cs/0205028}, 
}
@misc{rouge_score,
	author = {Google Research},
	title = {ROUGE Score Python Library},
	year = {2019},
	url = {https://github.com/google-research/google-research/tree/master/rouge},
	note = {Accedido el 27 de mayo de 2025}
}
